{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972c9428",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional, Union\n",
    "from dataclasses import dataclass\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    Wav2Vec2Model, Wav2Vec2Processor,\n",
    "    WhisperModel, WhisperProcessor, \n",
    "    BertModel, BertTokenizer,\n",
    "    AutoModel, AutoTokenizer\n",
    ")\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.metrics import accuracy_score, f1_score, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.stats import ks_2samp\n",
    "import scipy.linalg\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f91341",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ADReSSConfig:\n",
    "    \"\"\"Configuration class for ADReSS dataset processing.\"\"\"\n",
    "    # train_path: str = \"ADReSS-IS2020-train/ADReSS-IS2020-data/train\"\n",
    "    train_path: str = \"/kaggle/input/adress/adress/train\"\n",
    "    # test_path: str = \"ADReSS-IS2020-test/ADReSS-IS2020-data/test\"\n",
    "    test_path: str = \"/kaggle/input/adress/adress/test\"\n",
    "    audio_sr: int = 16000\n",
    "    \n",
    "    # Audio feature extraction parameters\n",
    "    n_mfcc: int = 13\n",
    "    n_mel: int = 128\n",
    "    hop_length: int = 512\n",
    "    n_fft: int = 2048\n",
    "    \n",
    "    # Feature statistics windows\n",
    "    statistical_windows: List[str] = None\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.statistical_windows is None:\n",
    "            self.statistical_windows = ['mean', 'std', 'min', 'max', 'median']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a486ebab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetadataLoader:\n",
    "    \"\"\"Handles loading and processing of metadata files for both training and test datasets.\"\"\"\n",
    "    \n",
    "    def __init__(self, base_path: str):\n",
    "        self.base_path = Path(base_path)\n",
    "        self.dataset_type = self._detect_dataset_type()\n",
    "    \n",
    "    def _detect_dataset_type(self) -> str:\n",
    "        \"\"\"Detect if this is training data or test data based on file structure.\"\"\"\n",
    "        cc_path = self.base_path / \"cc_meta_data.txt\"\n",
    "        cd_path = self.base_path / \"cd_meta_data.txt\"\n",
    "        test_path = self.base_path / \"meta_data.txt\"\n",
    "        \n",
    "        if cc_path.exists() and cd_path.exists():\n",
    "            return \"training\"\n",
    "        elif test_path.exists():\n",
    "            return \"test\"\n",
    "        else:\n",
    "            raise FileNotFoundError(\"Could not detect dataset type. Expected either training files (cc_meta_data.txt, cd_meta_data.txt) or test file (meta_data.txt)\")\n",
    "    \n",
    "    def load_metadata(self) -> pd.DataFrame:\n",
    "        \"\"\"Load metadata from either training or test dataset.\"\"\"\n",
    "        if self.dataset_type == \"training\":\n",
    "            return self._load_training_metadata()\n",
    "        else:\n",
    "            return self._load_test_metadata()\n",
    "    \n",
    "    def _load_training_metadata(self) -> pd.DataFrame:\n",
    "        \"\"\"Load metadata from both control and dementia groups.\"\"\"\n",
    "        # print(\"Loading training metadata...\")\n",
    "        cc_path = self.base_path / \"cc_meta_data.txt\"\n",
    "        cd_path = self.base_path / \"cd_meta_data.txt\"\n",
    "        \n",
    "        cc_df = pd.read_csv(cc_path, sep=';', skipinitialspace=True)\n",
    "        cd_df = pd.read_csv(cd_path, sep=';', skipinitialspace=True)\n",
    "        \n",
    "        # Clean column names by stripping whitespace\n",
    "        cc_df.columns = cc_df.columns.str.strip()\n",
    "        cd_df.columns = cd_df.columns.str.strip()\n",
    "        \n",
    "        cc_df['label'] = 0  # Control\n",
    "        cd_df['label'] = 1  # Dementia\n",
    "        cc_df['group'] = 'cc'\n",
    "        cd_df['group'] = 'cd'\n",
    "        \n",
    "        metadata = pd.concat([cc_df, cd_df], ignore_index=True)\n",
    "        \n",
    "        # Process IDs and MMSE\n",
    "        metadata['ID'] = metadata['ID'].str.strip()\n",
    "        metadata['mmse'] = pd.to_numeric(metadata['mmse'], errors='coerce')\n",
    "        \n",
    "        return metadata\n",
    "    \n",
    "    def _load_test_metadata(self) -> pd.DataFrame:\n",
    "        \"\"\"Load test metadata (no labels, different format).\"\"\"\n",
    "        # print(\"Loading test metadata...\")\n",
    "        meta_path = self.base_path / \"meta_data.txt\"\n",
    "        \n",
    "        # Read test metadata - format: ID ; age ; gender\n",
    "        test_df = pd.read_csv(meta_path, sep=';', skipinitialspace=True, header=None)\n",
    "        test_df.columns = ['ID', 'age', 'gender']\n",
    "        \n",
    "        # print(\"Processing test metadata...\")\n",
    "        # Clean column data\n",
    "        test_df['ID'] = test_df['ID'].str.strip()\n",
    "        test_df['age'] = pd.to_numeric(test_df['age'], errors='coerce')\n",
    "        test_df['gender'] = test_df['gender'].str.strip()\n",
    "        \n",
    "        # Test data has no labels or MMSE scores\n",
    "        test_df['label'] = -1  # Unknown label for test data\n",
    "        test_df['mmse'] = np.nan  # No MMSE scores in test data\n",
    "        test_df['group'] = 'test'\n",
    "        \n",
    "        return test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b54a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranscriptionProcessor:\n",
    "    \"\"\"Processes CHAT format transcription files.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.participant_pattern = re.compile(r'\\*PAR:\\s*(.*?)\\s*(\\d+)_(\\d+)')\n",
    "        self.investigator_pattern = re.compile(r'\\*INV:\\s*(.*?)\\s*(\\d+)_(\\d+)')\n",
    "        \n",
    "    def extract_speech_with_timing(self, file_path: str) -> Dict:\n",
    "        \"\"\"Extract participant and investigator speech with timestamps.\"\"\"\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        participant_utterances = []\n",
    "        investigator_utterances = []\n",
    "        participant_segments = []\n",
    "        investigator_segments = []\n",
    "        \n",
    "        # Find all matches first\n",
    "        par_matches = list(self.participant_pattern.finditer(content))\n",
    "        inv_matches = list(self.investigator_pattern.finditer(content))\n",
    "        \n",
    "        # Process participant matches\n",
    "        for match in par_matches:\n",
    "            utterance = self._clean_utterance(match.group(1))\n",
    "            start_time = int(match.group(2))\n",
    "            end_time = int(match.group(3))\n",
    "            if utterance:\n",
    "                participant_utterances.append(utterance)\n",
    "                participant_segments.append((start_time, end_time, utterance))\n",
    "        \n",
    "        # Process investigator matches\n",
    "        for match in inv_matches:\n",
    "            utterance = self._clean_utterance(match.group(1))\n",
    "            start_time = int(match.group(2))\n",
    "            end_time = int(match.group(3))\n",
    "            if utterance:\n",
    "                investigator_utterances.append(utterance)\n",
    "                investigator_segments.append((start_time, end_time, utterance))\n",
    "        \n",
    "        return {\n",
    "            'participant_text': ' '.join(participant_utterances),\n",
    "            'investigator_text': ' '.join(investigator_utterances),\n",
    "            'total_utterances': len(participant_utterances),\n",
    "            'participant_segments': participant_segments,\n",
    "            'investigator_segments': investigator_segments\n",
    "        }\n",
    "    \n",
    "    def extract_speech(self, file_path: str) -> Dict[str, str]:\n",
    "        \"\"\"Backward compatibility method.\"\"\"\n",
    "        return self.extract_speech_with_timing(file_path)\n",
    "    \n",
    "    def _clean_utterance(self, utterance: str) -> str:\n",
    "        \"\"\"Clean CHAT annotations from utterance.\"\"\"\n",
    "        cleaned = re.sub(r'\\[.*?\\]', '', utterance)\n",
    "        cleaned = re.sub(r'&\\w+', '', cleaned)\n",
    "        cleaned = re.sub(r'<[^>]*>', '', cleaned)\n",
    "        cleaned = re.sub(r'\\([^)]*\\)', '', cleaned)\n",
    "        cleaned = re.sub(r'\\s+', ' ', cleaned)\n",
    "        return cleaned.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611aa01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioProcessor:\n",
    "    \"\"\"Processes audio chunks and extracts features.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: ADReSSConfig):\n",
    "        self.config = config\n",
    "    \n",
    "    def load_audio_chunks(self, chunks_dir: str, subject_id: str) -> List[Dict]:\n",
    "        \"\"\"Load all normalized audio chunks for a subject. All chunks are participant speech.\"\"\"\n",
    "        chunks_path = Path(chunks_dir)\n",
    "        pattern = f\"{subject_id}-*.wav\"\n",
    "        chunk_files = list(chunks_path.glob(pattern))\n",
    "        \n",
    "        participant_chunks = []\n",
    "        \n",
    "        for chunk_file in chunk_files:\n",
    "            filename = chunk_file.name\n",
    "            audio, sr = librosa.load(str(chunk_file), sr=self.config.audio_sr)\n",
    "            \n",
    "            # Parse timing information from filename for sequencing\n",
    "            # Format: S001-7-4266-13310-1-2400-3470.wav\n",
    "            parts = filename.replace('.wav', '').split('-')\n",
    "            start_time = None\n",
    "            end_time = None\n",
    "            chunk_num = None\n",
    "            \n",
    "            if len(parts) >= 5:\n",
    "                try:\n",
    "                    start_time = int(parts[2])\n",
    "                    end_time = int(parts[3])\n",
    "                    chunk_num = int(parts[4]) if len(parts) > 4 else 0\n",
    "                except ValueError:\n",
    "                    pass\n",
    "            \n",
    "            chunk_info = {\n",
    "                'audio': audio,\n",
    "                'sr': sr,\n",
    "                'filename': filename,\n",
    "                'file_path': str(chunk_file),\n",
    "                'start_time': start_time,\n",
    "                'end_time': end_time,\n",
    "                'chunk_num': chunk_num\n",
    "            }\n",
    "            participant_chunks.append(chunk_info)\n",
    "        \n",
    "        # Sort chunks by temporal order (start_time, then chunk_num)\n",
    "        participant_chunks.sort(key=lambda x: (x['start_time'] or 0, x['chunk_num'] or 0))\n",
    "        \n",
    "        return participant_chunks\n",
    "    \n",
    "    def aggregate_chunk_features(self, chunks: List[Dict]) -> Dict[str, float]:\n",
    "        \"\"\"Return minimal info without handcrafted features.\"\"\"\n",
    "        if not chunks:\n",
    "            return {}\n",
    "        \n",
    "        total_duration = 0\n",
    "        valid_chunks = 0\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            audio = chunk['audio']\n",
    "            if len(audio) > 0:  # Skip empty audio chunks\n",
    "                total_duration += len(audio) / self.config.audio_sr\n",
    "                valid_chunks += 1\n",
    "        \n",
    "        return {\n",
    "            'total_duration': total_duration,\n",
    "            'num_chunks': valid_chunks\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac158c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlignmentProcessor:\n",
    "    \"\"\"Handles temporal alignment between transcript and audio chunks.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def align_transcript_audio(self, participant_segments: List[Tuple], \n",
    "                             participant_chunks: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Align transcript segments with audio chunks by timestamp overlap.\"\"\"\n",
    "        aligned_pairs = []\n",
    "        \n",
    "        # Process segments\n",
    "        for seg_start, seg_end, utterance in participant_segments:\n",
    "            for chunk in participant_chunks:\n",
    "                chunk_start = chunk.get('start_time')\n",
    "                chunk_end = chunk.get('end_time')\n",
    "                \n",
    "                if chunk_start is None or chunk_end is None:\n",
    "                    continue\n",
    "                \n",
    "                # Check for overlap\n",
    "                if not (seg_end <= chunk_start or seg_start >= chunk_end):\n",
    "                    aligned_pairs.append({\n",
    "                        'text': utterance,\n",
    "                        'audio_file': chunk['filename'],\n",
    "                        'text_start': seg_start,\n",
    "                        'text_end': seg_end,\n",
    "                        'audio_start': chunk_start,\n",
    "                        'audio_end': chunk_end,\n",
    "                        'audio_data': chunk['audio']\n",
    "                    })\n",
    "        \n",
    "        return aligned_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161da8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ADReSSDataLoader:\n",
    "    \"\"\"Main data loader class that orchestrates all preprocessing components for both training and test data.\"\"\"\n",
    "    \n",
    "    def __init__(self, base_path: str, config: Optional[ADReSSConfig] = None):\n",
    "        self.base_path = Path(base_path)\n",
    "        self.config = config or ADReSSConfig()\n",
    "        \n",
    "        self.metadata_loader = MetadataLoader(self.base_path)\n",
    "        self.transcription_processor = TranscriptionProcessor()\n",
    "        self.audio_processor = AudioProcessor(self.config)\n",
    "        self.alignment_processor = AlignmentProcessor()\n",
    "        \n",
    "        # Detect dataset type\n",
    "        self.dataset_type = self.metadata_loader.dataset_type\n",
    "        \n",
    "    def load_dataset(self, \n",
    "                    use_audio_chunks: bool = True,\n",
    "                    include_audio: bool = True,\n",
    "                    include_text: bool = True,\n",
    "                    align_modalities: bool = False) -> pd.DataFrame:\n",
    "        \"\"\"Load complete dataset with optional modality alignment.\"\"\"\n",
    "        \n",
    "        metadata = self.metadata_loader.load_metadata()\n",
    "        dataset_rows = []\n",
    "        \n",
    "        pbar_desc = f\"Processing {self.dataset_type} subjects\"\n",
    "        for _, row in tqdm(metadata.iterrows(), desc=pbar_desc, total=len(metadata), leave=False):\n",
    "            subject_id = row['ID']\n",
    "            group = row['group']\n",
    "            \n",
    "            sample_data = {\n",
    "                'subject_id': subject_id,\n",
    "                'age': row['age'],\n",
    "                'gender': row['gender'],\n",
    "                'mmse': row['mmse'],\n",
    "                'label': row['label'],\n",
    "                'group': group\n",
    "            }\n",
    "            \n",
    "            # Process transcription and audio\n",
    "            if align_modalities and include_text and include_audio:\n",
    "                aligned_data = self._get_aligned_data(subject_id, group)\n",
    "                sample_data.update(aligned_data)\n",
    "            else:\n",
    "                # Process transcription\n",
    "                if include_text:\n",
    "                    transcript_path = self._get_transcript_path(subject_id, group)\n",
    "                    if transcript_path and transcript_path.exists():\n",
    "                        text_data = self.transcription_processor.extract_speech_with_timing(str(transcript_path))\n",
    "                        sample_data.update({k: v for k, v in text_data.items() \n",
    "                                          if k not in ['participant_segments', 'investigator_segments']})\n",
    "                \n",
    "                # Process audio chunks\n",
    "                if include_audio and use_audio_chunks:\n",
    "                    chunks_dir = self._get_chunks_directory(group)\n",
    "                    if chunks_dir and chunks_dir.exists():\n",
    "                        try:\n",
    "                            participant_chunks = self.audio_processor.load_audio_chunks(\n",
    "                                str(chunks_dir), subject_id\n",
    "                            )\n",
    "                            \n",
    "                            if participant_chunks:\n",
    "                                basic_audio_info = self.audio_processor.aggregate_chunk_features(participant_chunks)\n",
    "                                for key, value in basic_audio_info.items():\n",
    "                                    sample_data[f'participant_{key}'] = value\n",
    "                            \n",
    "                            sample_data['total_participant_chunks'] = len(participant_chunks)\n",
    "                            \n",
    "                        except Exception as e:\n",
    "                            print(f\"Error processing audio chunks for {subject_id}: {e}\")\n",
    "                            sample_data['total_participant_chunks'] = 0\n",
    "            \n",
    "            dataset_rows.append(sample_data)\n",
    "        \n",
    "        return pd.DataFrame(dataset_rows)\n",
    "    \n",
    "    def _get_aligned_data(self, subject_id: str, group: str) -> Dict:\n",
    "        \"\"\"Get aligned text-audio pairs for a subject.\"\"\"\n",
    "        # Get transcript segments\n",
    "        transcript_path = self._get_transcript_path(subject_id, group)\n",
    "        if not transcript_path or not transcript_path.exists():\n",
    "            return {}\n",
    "        \n",
    "        text_data = self.transcription_processor.extract_speech_with_timing(str(transcript_path))\n",
    "        participant_segments = text_data['participant_segments']\n",
    "        \n",
    "        # Get audio chunks\n",
    "        chunks_dir = self._get_chunks_directory(group)\n",
    "        if not chunks_dir or not chunks_dir.exists():\n",
    "            return {}\n",
    "        \n",
    "        participant_chunks = self.audio_processor.load_audio_chunks(str(chunks_dir), subject_id)\n",
    "        \n",
    "        # Use AlignmentProcessor to align segments with chunks\n",
    "        aligned_pairs = self.alignment_processor.align_transcript_audio(\n",
    "            participant_segments, participant_chunks\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'participant_text': text_data['participant_text'],\n",
    "            'investigator_text': text_data['investigator_text'],\n",
    "            'total_utterances': text_data['total_utterances'],\n",
    "            'aligned_pairs': aligned_pairs,\n",
    "            'num_aligned_pairs': len(aligned_pairs)\n",
    "        }\n",
    "    \n",
    "    def _get_transcript_path(self, subject_id: str, group: str) -> Optional[Path]:\n",
    "        \"\"\"Get transcript path based on dataset type.\"\"\"\n",
    "        if self.dataset_type == \"training\":\n",
    "            return self.base_path / \"transcription\" / group / f\"{subject_id}.cha\"\n",
    "        else:\n",
    "            return self.base_path / \"transcription\" / f\"{subject_id}.cha\"\n",
    "    \n",
    "    def _get_chunks_directory(self, group: str) -> Optional[Path]:\n",
    "        \"\"\"Get audio chunks directory based on dataset type.\"\"\"\n",
    "        if self.dataset_type == \"training\":\n",
    "            return self.base_path / \"Normalised_audio-chunks\" / group\n",
    "        else:\n",
    "            return self.base_path / \"Normalised_audio-chunks\"\n",
    "    \n",
    "    def get_subject_chunks(self, subject_id: str) -> Dict:\n",
    "        \"\"\"Get detailed chunk data for a specific subject.\"\"\"\n",
    "        metadata = self.metadata_loader.load_metadata()\n",
    "        subject_meta = metadata[metadata['ID'] == subject_id].iloc[0]\n",
    "        \n",
    "        group = subject_meta['group']\n",
    "        \n",
    "        # Load transcription with timing\n",
    "        transcript_path = self._get_transcript_path(subject_id, group)\n",
    "        transcript_data = {}\n",
    "        if transcript_path and transcript_path.exists():\n",
    "            transcript_data = self.transcription_processor.extract_speech_with_timing(str(transcript_path))\n",
    "        \n",
    "        # Load audio chunks\n",
    "        chunks_dir = self._get_chunks_directory(group)\n",
    "        participant_chunks = []\n",
    "        if chunks_dir and chunks_dir.exists():\n",
    "            participant_chunks = self.audio_processor.load_audio_chunks(str(chunks_dir), subject_id)\n",
    "        \n",
    "        return {\n",
    "            'metadata': subject_meta.to_dict(),\n",
    "            'transcription': transcript_data,\n",
    "            'participant_chunks': participant_chunks,\n",
    "            'chunks_dir': str(chunks_dir) if chunks_dir else None,\n",
    "            'transcript_path': str(transcript_path) if transcript_path else None,\n",
    "            'dataset_type': self.dataset_type\n",
    "        }\n",
    "    \n",
    "    def get_aligned_pairs(self, subject_id: str) -> List[Dict]:\n",
    "        \"\"\"Get aligned text-audio pairs for a specific subject.\"\"\"\n",
    "        metadata = self.metadata_loader.load_metadata()\n",
    "        subject_meta = metadata[metadata['ID'] == subject_id].iloc[0]\n",
    "        group = subject_meta['group']\n",
    "        \n",
    "        aligned_data = self._get_aligned_data(subject_id, group)\n",
    "        return aligned_data.get('aligned_pairs', [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854a93c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioEmbedder:\n",
    "    def __init__(self, model_type='wav2vec2', device='cuda'):\n",
    "        self.model_type = model_type\n",
    "        self.device = device\n",
    "        \n",
    "        if model_type == 'wav2vec2':\n",
    "            self.processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")\n",
    "            self.model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base\").to(device)\n",
    "        elif model_type == 'whisper':\n",
    "            self.processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\")\n",
    "            self.model = WhisperModel.from_pretrained(\"openai/whisper-small\").to(device)\n",
    "        \n",
    "        self.model.eval()\n",
    "        \n",
    "    def extract_embeddings(self, audio_chunks):\n",
    "        embeddings = []\n",
    "        \n",
    "        for i, chunk in enumerate(audio_chunks):\n",
    "            audio = chunk['audio']\n",
    "            \n",
    "            try:\n",
    "                if self.model_type == 'wav2vec2':\n",
    "                    inputs = self.processor(audio, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "                    inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        outputs = self.model(**inputs)\n",
    "                        embedding = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "                elif self.model_type == 'whisper':\n",
    "                    inputs = self.processor(audio, sampling_rate=16000, return_tensors=\"pt\")\n",
    "                    inputs = {k: v.to(self.device) if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        outputs = self.model.encoder(**inputs)\n",
    "                        embedding = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "                        \n",
    "                embeddings.append(embedding[0])\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing chunk: {e}\")\n",
    "                embeddings.append(np.zeros(768))\n",
    "                \n",
    "        return np.array(embeddings)\n",
    "\n",
    "class TextEmbedder:\n",
    "    def __init__(self, model_type='clinicalbert', device='cuda'):\n",
    "        self.model_type = model_type\n",
    "        self.device = device\n",
    "        \n",
    "        if model_type == 'clinicalbert':\n",
    "            model_name = \"emilyalsentzer/Bio_ClinicalBERT\"\n",
    "        elif model_type == 'biobert':\n",
    "            model_name = \"dmis-lab/biobert-v1.1\"\n",
    "        else:\n",
    "            model_name = \"bert-base-uncased\"\n",
    "            \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name).to(device)\n",
    "        self.model.eval()\n",
    "        \n",
    "    def extract_embeddings(self, texts):\n",
    "        embeddings = []\n",
    "        \n",
    "        for i, text in enumerate(texts):\n",
    "            try:\n",
    "                inputs = self.tokenizer(text, return_tensors=\"pt\", padding=True, \n",
    "                                      truncation=True, max_length=512)\n",
    "                inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    outputs = self.model(**inputs)\n",
    "                    embedding = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "                    \n",
    "                embeddings.append(embedding[0])\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing text: {e}\")\n",
    "                embeddings.append(np.zeros(768))\n",
    "                \n",
    "        return np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172575dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalFusion(nn.Module):\n",
    "    def __init__(self, audio_dim=768, text_dim=768, fusion_type='concat', output_dim=512, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.fusion_type = fusion_type\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        if fusion_type == 'concat':\n",
    "            self.fusion_layer = nn.Linear(audio_dim + text_dim, output_dim)\n",
    "        elif fusion_type == 'cross_attention':\n",
    "            self.audio_proj = nn.Linear(audio_dim, output_dim)\n",
    "            self.text_proj = nn.Linear(text_dim, output_dim)\n",
    "            self.cross_attn = nn.MultiheadAttention(output_dim, num_heads=num_heads, batch_first=True)\n",
    "            self.norm = nn.LayerNorm(output_dim)\n",
    "            self.fusion_layer = nn.Linear(output_dim * 2, output_dim)\n",
    "        \n",
    "    def forward(self, audio_emb, text_emb):\n",
    "        if self.fusion_type == 'concat':\n",
    "            combined = torch.cat([audio_emb, text_emb], dim=-1)\n",
    "            return self.fusion_layer(combined)\n",
    "        \n",
    "        elif self.fusion_type == 'cross_attention':\n",
    "            audio_proj = self.audio_proj(audio_emb).unsqueeze(1)\n",
    "            text_proj = self.text_proj(text_emb).unsqueeze(1)\n",
    "            \n",
    "            audio_attended, _ = self.cross_attn(audio_proj, text_proj, text_proj)\n",
    "            text_attended, _ = self.cross_attn(text_proj, audio_proj, audio_proj)\n",
    "            \n",
    "            audio_attended = self.norm(audio_attended.squeeze(1))\n",
    "            text_attended = self.norm(text_attended.squeeze(1))\n",
    "            \n",
    "            combined = torch.cat([audio_attended, text_attended], dim=-1)\n",
    "            return self.fusion_layer(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edba6f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GenerationConfig:\n",
    "    \"\"\"Configuration for text and audio generation.\"\"\"\n",
    "    # Text generation\n",
    "    text_model_name: str = \"gpt2\"\n",
    "    max_text_length: int = 256\n",
    "    text_temperature: float = 0.8\n",
    "    \n",
    "    # Audio generation  \n",
    "    tts_model_name: str = \"facebook/mms-tts-eng\"\n",
    "    sample_rate: int = 16000\n",
    "    \n",
    "    # Clinical conditioning\n",
    "    condition_dim: int = 16\n",
    "    num_samples_per_condition: int = 50\n",
    "    \n",
    "    # Training\n",
    "    epochs: int = 100\n",
    "    batch_size: int = 16\n",
    "    learning_rate: float = 2e-5\n",
    "\n",
    "class ClinicalTextGenerator(nn.Module):\n",
    "    def __init__(self, config: GenerationConfig):\n",
    "        super().__init__()\n",
    "        from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "        \n",
    "        self.config = config\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(config.text_model_name)\n",
    "        self.model = GPT2LMHeadModel.from_pretrained(config.text_model_name)\n",
    "        \n",
    "        # Add padding token\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        # Clinical condition embedding\n",
    "        self.condition_proj = nn.Linear(config.condition_dim, self.model.config.n_embd)\n",
    "        \n",
    "        # Freeze base model initially\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        # Only train condition projection and final layer\n",
    "        for param in self.model.lm_head.parameters():\n",
    "            param.requires_grad = True\n",
    "    \n",
    "    def create_ad_prompts(self):\n",
    "        \"\"\"Create AD-specific prompts based on common assessment tasks.\"\"\"\n",
    "        prompts = [\n",
    "            \"Tell me about your daily routine.\",\n",
    "            \"Describe what you see in this picture.\",\n",
    "            \"What did you do yesterday?\",\n",
    "            \"Tell me about your family.\",\n",
    "            \"Describe how to make a sandwich.\",\n",
    "            \"What is your favorite memory?\",\n",
    "            \"Tell me about your hometown.\",\n",
    "            \"Describe the changing seasons.\",\n",
    "            \"What do you like to do for fun?\",\n",
    "            \"Tell me about a typical day.\"\n",
    "        ]\n",
    "        return prompts\n",
    "    \n",
    "    def add_ad_characteristics(self, text, severity='mild'):\n",
    "        \"\"\"Add AD-specific linguistic patterns to generated text.\"\"\"\n",
    "        if severity == 'mild':\n",
    "            # Mild: slight word-finding issues, occasional repetition\n",
    "            text = re.sub(r'\\b(\\w+)\\b', lambda m: f\"{m.group(1)}... {m.group(1)}\" if np.random.random() < 0.1 else m.group(1), text)\n",
    "            text = re.sub(r'\\.', '... um...', text) if np.random.random() < 0.2 else text\n",
    "            \n",
    "        elif severity == 'moderate':\n",
    "            # Moderate: more pauses, word substitutions, incomplete sentences\n",
    "            text = re.sub(r'\\b\\w{4,}\\b', lambda m: \"thing\" if np.random.random() < 0.15 else m.group(0), text)\n",
    "            text = re.sub(r'\\.', '... what was I saying...', text) if np.random.random() < 0.3 else text\n",
    "            text = text.replace('.', '... ') if np.random.random() < 0.4 else text\n",
    "            \n",
    "        return text\n",
    "    \n",
    "    def generate_text(self, condition_vector, severity='mild'):\n",
    "        \"\"\"Generate AD-specific text conditioned on clinical markers.\"\"\"\n",
    "        prompts = self.create_ad_prompts()\n",
    "        prompt = np.random.choice(prompts)\n",
    "        \n",
    "        # Encode prompt\n",
    "        inputs = self.tokenizer(prompt, return_tensors='pt', padding=True)\n",
    "        input_ids = inputs['input_ids']\n",
    "        \n",
    "        # Add condition embedding to first token\n",
    "        with torch.no_grad():\n",
    "            condition_emb = self.condition_proj(condition_vector.unsqueeze(0))\n",
    "            \n",
    "            # Generate text\n",
    "            output = self.model.generate(\n",
    "                input_ids,\n",
    "                max_length=self.config.max_text_length,\n",
    "                temperature=self.config.text_temperature,\n",
    "                do_sample=True,\n",
    "                pad_token_id=self.tokenizer.eos_token_id,\n",
    "                no_repeat_ngram_size=2\n",
    "            )\n",
    "            \n",
    "        # Decode and process\n",
    "        generated_text = self.tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        generated_text = generated_text.replace(prompt, \"\").strip()\n",
    "        \n",
    "        # Add AD characteristics based on severity\n",
    "        if severity in ['mild', 'moderate']:\n",
    "            generated_text = self.add_ad_characteristics(generated_text, severity)\n",
    "            \n",
    "        return generated_text\n",
    "\n",
    "class ClinicalSpeechSynthesizer:\n",
    "    def __init__(self, config: GenerationConfig):\n",
    "        self.config = config\n",
    "        \n",
    "        # Use a simpler TTS approach that's more controllable\n",
    "        import pyttsx3\n",
    "        self.tts_engine = pyttsx3.init()\n",
    "        \n",
    "        # Set voice properties for clinical realism\n",
    "        voices = self.tts_engine.getProperty('voices')\n",
    "        if voices:\n",
    "            self.tts_engine.setProperty('voice', voices[0].id)\n",
    "    \n",
    "    def modify_speech_for_ad(self, text, severity='mild', age_group='elderly'):\n",
    "        \"\"\"Add pauses and modify speech rate for AD characteristics.\"\"\"\n",
    "        \n",
    "        # Add more pauses for AD patients\n",
    "        if severity == 'mild':\n",
    "            speech_rate = 140  # slightly slower\n",
    "            text = re.sub(r'\\.', '. <break time=\"0.8s\"/>', text)\n",
    "            text = re.sub(r',', ', <break time=\"0.3s\"/>', text)\n",
    "            \n",
    "        elif severity == 'moderate':\n",
    "            speech_rate = 110  # noticeably slower\n",
    "            text = re.sub(r'\\.', '. <break time=\"1.2s\"/>', text)\n",
    "            text = re.sub(r',', ', <break time=\"0.5s\"/>', text)\n",
    "            text = re.sub(r'\\s+', ' <break time=\"0.2s\"/>', text)  # More frequent pauses\n",
    "            \n",
    "        else:  # healthy\n",
    "            speech_rate = 160\n",
    "            \n",
    "        # Adjust for age\n",
    "        if age_group == 'elderly':\n",
    "            speech_rate = max(speech_rate - 20, 100)\n",
    "            \n",
    "        return text, speech_rate\n",
    "    \n",
    "    def text_to_audio(self, text, condition_vector):\n",
    "        \"\"\"Convert text to audio with clinical characteristics.\"\"\"\n",
    "        # Extract clinical info from condition vector\n",
    "        age = condition_vector[0].item()\n",
    "        mmse = condition_vector[2].item() if len(condition_vector) > 2 else 0.8\n",
    "        \n",
    "        # Determine severity and age group\n",
    "        if mmse > 0.8:\n",
    "            severity = 'healthy'\n",
    "        elif mmse > 0.6:\n",
    "            severity = 'mild'\n",
    "        else:\n",
    "            severity = 'moderate'\n",
    "            \n",
    "        age_group = 'elderly' if age > 0.7 else 'adult'\n",
    "        \n",
    "        # Modify text for AD characteristics\n",
    "        modified_text, speech_rate = self.modify_speech_for_ad(text, severity, age_group)\n",
    "        \n",
    "        # Set speech properties\n",
    "        self.tts_engine.setProperty('rate', speech_rate)\n",
    "        \n",
    "        # Generate audio to temporary file\n",
    "        import tempfile\n",
    "        with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as tmp_file:\n",
    "            self.tts_engine.save_to_file(modified_text, tmp_file.name)\n",
    "            self.tts_engine.runAndWait()\n",
    "            \n",
    "            # Load and return audio\n",
    "            audio, sr = librosa.load(tmp_file.name, sr=self.config.sample_rate)\n",
    "            \n",
    "        return audio\n",
    "\n",
    "class AlignedDataGenerator:\n",
    "    def __init__(self, config: GenerationConfig, device='cuda'):\n",
    "        self.config = config\n",
    "        self.device = device\n",
    "        \n",
    "        self.text_generator = ClinicalTextGenerator(config)\n",
    "        self.speech_synthesizer = ClinicalSpeechSynthesizer(config)\n",
    "        \n",
    "    def create_clinical_condition(self, label, age_group='mixed'):\n",
    "        \"\"\"Create condition vector with clinical realism.\"\"\"\n",
    "        condition = torch.zeros(self.config.condition_dim)\n",
    "        \n",
    "        # Age (0-1 normalized)\n",
    "        if age_group == 'young':\n",
    "            age = np.random.uniform(0.2, 0.4)\n",
    "        elif age_group == 'elderly':\n",
    "            age = np.random.uniform(0.7, 0.95)\n",
    "        else:\n",
    "            age = np.random.uniform(0.4, 0.9)\n",
    "        condition[0] = age\n",
    "        \n",
    "        # Gender (0=F, 1=M)\n",
    "        condition[1] = np.random.choice([0, 1])\n",
    "        \n",
    "        # MMSE (0-1 normalized, realistic based on label)\n",
    "        if label == 0:  # Healthy\n",
    "            mmse = np.random.uniform(0.8, 1.0)\n",
    "        else:  # AD\n",
    "            mmse = np.random.uniform(0.1, 0.7)\n",
    "        condition[2] = mmse\n",
    "        \n",
    "        # Additional clinical markers\n",
    "        condition[3] = label  # AD status\n",
    "        condition[4:8] = torch.randn(4) * 0.1  # Other clinical factors\n",
    "        \n",
    "        # Demographics and audio characteristics\n",
    "        condition[8:] = torch.randn(self.config.condition_dim - 8) * 0.2\n",
    "        \n",
    "        return condition\n",
    "    \n",
    "    def generate_aligned_sample(self, label):\n",
    "        \"\"\"Generate a single aligned text-audio pair.\"\"\"\n",
    "        # Create clinical condition\n",
    "        condition = self.create_clinical_condition(label)\n",
    "        \n",
    "        # Determine AD severity from condition\n",
    "        mmse = condition[2].item()\n",
    "        if mmse > 0.8:\n",
    "            severity = 'healthy'\n",
    "        elif mmse > 0.6:\n",
    "            severity = 'mild'\n",
    "        else:\n",
    "            severity = 'moderate'\n",
    "        \n",
    "        # Generate text\n",
    "        generated_text = self.text_generator.generate_text(condition, severity)\n",
    "        \n",
    "        # Generate corresponding audio\n",
    "        audio = self.speech_synthesizer.text_to_audio(generated_text, condition)\n",
    "        \n",
    "        return {\n",
    "            'text': generated_text,\n",
    "            'audio': audio,\n",
    "            'label': label,\n",
    "            'condition': condition.numpy(),\n",
    "            'mmse': mmse,\n",
    "            'severity': severity\n",
    "        }\n",
    "    \n",
    "    def generate_balanced_dataset(self, samples_per_class=100):\n",
    "        \"\"\"Generate balanced dataset of aligned text-audio pairs.\"\"\"\n",
    "        synthetic_data = []\n",
    "        \n",
    "        print(f\"Generating {samples_per_class * 2} aligned text-audio pairs...\")\n",
    "        \n",
    "        with tqdm(total=samples_per_class * 2, desc=\"Generating samples\") as pbar:\n",
    "            for label in [0, 1]:  # Healthy, AD\n",
    "                for i in range(samples_per_class):\n",
    "                    try:\n",
    "                        sample = self.generate_aligned_sample(label)\n",
    "                        synthetic_data.append(sample)\n",
    "                        pbar.update(1)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error generating sample: {e}\")\n",
    "                        continue\n",
    "        \n",
    "        return synthetic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bab2e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SyntheticDatasetProcessor:\n",
    "    def __init__(self, audio_embedder, text_embedder, fusion_model):\n",
    "        self.audio_embedder = audio_embedder\n",
    "        self.text_embedder = text_embedder\n",
    "        self.fusion_model = fusion_model\n",
    "    \n",
    "    def process_synthetic_data(self, synthetic_samples):\n",
    "        \"\"\"Convert synthetic text-audio pairs to embeddings for training.\"\"\"\n",
    "        processed_data = []\n",
    "        labels = []\n",
    "        \n",
    "        print(\"Processing synthetic samples for training...\")\n",
    "        \n",
    "        for sample in tqdm(synthetic_samples, desc=\"Processing\"):\n",
    "            try:\n",
    "                # Convert audio to chunks format expected by embedder\n",
    "                audio_chunks = [{'audio': sample['audio']}]\n",
    "                \n",
    "                # Extract embeddings\n",
    "                audio_emb = self.audio_embedder.extract_embeddings(audio_chunks).mean(axis=0)\n",
    "                text_emb = self.text_embedder.extract_embeddings([sample['text']])[0]\n",
    "                \n",
    "                # Create fused embedding\n",
    "                audio_tensor = torch.tensor(audio_emb, dtype=torch.float32)\n",
    "                text_tensor = torch.tensor(text_emb, dtype=torch.float32)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    fused_emb = self.fusion_model(audio_tensor.unsqueeze(0), text_tensor.unsqueeze(0))\n",
    "                \n",
    "                processed_data.append(fused_emb.squeeze(0).numpy())\n",
    "                labels.append(sample['label'])\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing sample: {e}\")\n",
    "                continue\n",
    "        \n",
    "        return np.array(processed_data), np.array(labels)\n",
    "\n",
    "def evaluate_synthetic_quality(original_data, synthetic_data, labels):\n",
    "    \"\"\"Evaluate quality of synthetic data.\"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    # Frechet distance\n",
    "    mu_real = np.mean(original_data, axis=0)\n",
    "    mu_synth = np.mean(synthetic_data, axis=0)\n",
    "    sigma_real = np.cov(original_data.T)\n",
    "    sigma_synth = np.cov(synthetic_data.T)\n",
    "    \n",
    "    diff = mu_real - mu_synth\n",
    "    covmean = scipy.linalg.sqrtm(sigma_real.dot(sigma_synth))\n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "    \n",
    "    fid = diff.dot(diff) + np.trace(sigma_real) + np.trace(sigma_synth) - 2 * np.trace(covmean)\n",
    "    metrics['frechet_distance'] = fid\n",
    "    \n",
    "    # Label distribution\n",
    "    unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "    label_dist = dict(zip(unique_labels, counts))\n",
    "    metrics['label_distribution'] = label_dist\n",
    "    \n",
    "    # KS test for distribution similarity\n",
    "    ks_stats = []\n",
    "    for dim in range(min(original_data.shape[1], synthetic_data.shape[1])):\n",
    "        stat, _ = ks_2samp(original_data[:, dim], synthetic_data[:, dim])\n",
    "        ks_stats.append(stat)\n",
    "    metrics['ks_statistic'] = np.mean(ks_stats)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "class ExperimentPipeline:\n",
    "    def __init__(self, train_loader, test_loader, device='cuda'):\n",
    "        self.train_loader = train_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.device = device\n",
    "        \n",
    "    def run_generation_experiment(self, audio_model='wav2vec2', text_model='clinicalbert', \n",
    "                                fusion_type='cross_attention', samples_per_class=100):\n",
    "        \"\"\"Run complete text→audio generation experiment.\"\"\"\n",
    "        \n",
    "        config_name = f\"{audio_model}_{text_model}_{fusion_type}_generation\"\n",
    "        print(f\"\\\\nRunning generation experiment: {config_name}\")\n",
    "        \n",
    "        # Initialize models\n",
    "        audio_embedder = AudioEmbedder(audio_model, self.device)\n",
    "        text_embedder = TextEmbedder(text_model, self.device)\n",
    "        fusion_model = MultimodalFusion(\n",
    "            fusion_type=fusion_type, \n",
    "            output_dim=512\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Create baseline dataset\n",
    "        train_dataset = self._create_baseline_dataset(audio_embedder, text_embedder, fusion_model)\n",
    "        \n",
    "        # Baseline performance\n",
    "        baseline_results = self._evaluate_baseline(train_dataset)\n",
    "        \n",
    "        # Generate synthetic data\n",
    "        generation_config = GenerationConfig(num_samples_per_condition=samples_per_class)\n",
    "        data_generator = AlignedDataGenerator(generation_config, self.device)\n",
    "        synthetic_samples = data_generator.generate_balanced_dataset(samples_per_class)\n",
    "        \n",
    "        # Process synthetic data\n",
    "        processor = SyntheticDatasetProcessor(audio_embedder, text_embedder, fusion_model)\n",
    "        synthetic_embeddings, synthetic_labels = processor.process_synthetic_data(synthetic_samples)\n",
    "        \n",
    "        # Evaluate synthetic data quality\n",
    "        quality_metrics = evaluate_synthetic_quality(train_dataset.data, synthetic_embeddings, synthetic_labels)\n",
    "        \n",
    "        # Train with augmented data\n",
    "        augmented_results = self._evaluate_with_augmentation(\n",
    "            train_dataset, synthetic_embeddings, synthetic_labels, baseline_results['split_data']\n",
    "        )\n",
    "        \n",
    "        # Generate test predictions\n",
    "        test_results = self._generate_test_predictions(\n",
    "            audio_embedder, text_embedder, fusion_model,\n",
    "            augmented_results['model'], augmented_results['scaler'], config_name\n",
    "        )\n",
    "        \n",
    "        # Compile results\n",
    "        improvement = augmented_results['accuracy'] - baseline_results['accuracy']\n",
    "        \n",
    "        result = {\n",
    "            'config': config_name,\n",
    "            'audio_model': audio_model,\n",
    "            'text_model': text_model,\n",
    "            'fusion_type': fusion_type,\n",
    "            'baseline_accuracy': baseline_results['accuracy'],\n",
    "            'baseline_f1': baseline_results['f1_score'],\n",
    "            'baseline_cv': f\"{baseline_results['cv_mean']:.3f}±{baseline_results['cv_std']:.3f}\",\n",
    "            'augmented_accuracy': augmented_results['accuracy'],\n",
    "            'augmented_f1': augmented_results['f1_score'],\n",
    "            'augmented_cv': f\"{augmented_results['cv_mean']:.3f}±{augmented_results['cv_std']:.3f}\",\n",
    "            'improvement': improvement,\n",
    "            'num_synthetic_samples': len(synthetic_samples),\n",
    "            **quality_metrics\n",
    "        }\n",
    "        \n",
    "        print(f\"Results for {config_name}:\")\n",
    "        print(f\"  Generated {len(synthetic_samples)} aligned text-audio pairs\")\n",
    "        print(f\"  Baseline Acc: {baseline_results['accuracy']:.3f}\")\n",
    "        print(f\"  Augmented Acc: {augmented_results['accuracy']:.3f}\")\n",
    "        print(f\"  Improvement: {improvement:+.3f}\")\n",
    "        print(f\"  Synthetic Quality (FID): {quality_metrics['frechet_distance']:.3f}\")\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def _create_baseline_dataset(self, audio_embedder, text_embedder, fusion_model):\n",
    "        \"\"\"Create baseline dataset from real data.\"\"\"\n",
    "        embeddings = []\n",
    "        labels = []\n",
    "        \n",
    "        dataset = self.train_loader.load_dataset(align_modalities=True)\n",
    "        \n",
    "        for _, row in tqdm(dataset.iterrows(), desc=\"Creating baseline dataset\", total=len(dataset), leave=False):\n",
    "            try:\n",
    "                subject_chunks = self.train_loader.get_subject_chunks(row['subject_id'])\n",
    "                audio_chunks = subject_chunks['participant_chunks']\n",
    "                text = row['participant_text']\n",
    "                \n",
    "                if audio_chunks and text and len(text.strip()) > 0:\n",
    "                    audio_emb = audio_embedder.extract_embeddings(audio_chunks).mean(axis=0)\n",
    "                    text_emb = text_embedder.extract_embeddings([text])[0]\n",
    "                    \n",
    "                    audio_tensor = torch.tensor(audio_emb, dtype=torch.float32).to(self.device)\n",
    "                    text_tensor = torch.tensor(text_emb, dtype=torch.float32).to(self.device)\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        fused_emb = fusion_model(audio_tensor.unsqueeze(0), text_tensor.unsqueeze(0))\n",
    "                    \n",
    "                    embeddings.append(fused_emb.squeeze(0).cpu().numpy())\n",
    "                    labels.append(int(row['label']))\n",
    "                    \n",
    "            except Exception:\n",
    "                continue\n",
    "        \n",
    "        class SimpleDataset:\n",
    "            def __init__(self, data, labels):\n",
    "                self.data = np.array(data)\n",
    "                self.labels = np.array(labels)\n",
    "        \n",
    "        return SimpleDataset(embeddings, labels)\n",
    "    \n",
    "    def _evaluate_baseline(self, dataset):\n",
    "        \"\"\"Evaluate baseline performance.\"\"\"\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            dataset.data, dataset.labels, test_size=0.2, \n",
    "            stratify=dataset.labels, random_state=42\n",
    "        )\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_val_scaled = scaler.transform(X_val)\n",
    "        \n",
    "        svm_clf = SVC(C=2.0, kernel='rbf', probability=True, random_state=42)\n",
    "        svm_clf.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        y_pred = svm_clf.predict(X_val_scaled)\n",
    "        accuracy = accuracy_score(y_val, y_pred)\n",
    "        f1 = f1_score(y_val, y_pred)\n",
    "        \n",
    "        # Cross-validation\n",
    "        skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "        cv_scores = []\n",
    "        \n",
    "        for train_idx, val_idx in skf.split(X_train, y_train):\n",
    "            X_cv_train, X_cv_val = X_train[train_idx], X_train[val_idx]\n",
    "            y_cv_train, y_cv_val = y_train[train_idx], y_train[val_idx]\n",
    "            \n",
    "            X_cv_train_scaled = scaler.fit_transform(X_cv_train)\n",
    "            X_cv_val_scaled = scaler.transform(X_cv_val)\n",
    "            \n",
    "            svm_cv = SVC(C=2.0, kernel='rbf', random_state=42)\n",
    "            svm_cv.fit(X_cv_train_scaled, y_cv_train)\n",
    "            cv_scores.append(accuracy_score(y_cv_val, svm_cv.predict(X_cv_val_scaled)))\n",
    "        \n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'f1_score': f1,\n",
    "            'cv_mean': np.mean(cv_scores),\n",
    "            'cv_std': np.std(cv_scores),\n",
    "            'split_data': (X_train, X_val, y_train, y_val),\n",
    "            'scaler': scaler,\n",
    "            'model': svm_clf\n",
    "        }\n",
    "    \n",
    "    def _evaluate_with_augmentation(self, dataset, synthetic_embeddings, synthetic_labels, baseline_split):\n",
    "        \"\"\"Evaluate with synthetic data augmentation.\"\"\"\n",
    "        X_train, X_val, y_train, y_val = baseline_split\n",
    "        \n",
    "        # Augment training data\n",
    "        X_aug = np.concatenate([X_train, synthetic_embeddings])\n",
    "        y_aug = np.concatenate([y_train, synthetic_labels])\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        X_aug_scaled = scaler.fit_transform(X_aug)\n",
    "        X_val_scaled = scaler.transform(X_val)\n",
    "        \n",
    "        svm_aug = SVC(C=2.0, kernel='rbf', probability=True, random_state=42)\n",
    "        svm_aug.fit(X_aug_scaled, y_aug)\n",
    "        \n",
    "        y_pred = svm_aug.predict(X_val_scaled)\n",
    "        accuracy = accuracy_score(y_val, y_pred)\n",
    "        f1 = f1_score(y_val, y_pred)\n",
    "        \n",
    "        # Cross-validation\n",
    "        skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "        cv_scores = []\n",
    "        \n",
    "        syn_subset_size = min(len(synthetic_embeddings), len(X_train) // 2)\n",
    "        syn_indices = np.random.choice(len(synthetic_embeddings), syn_subset_size, replace=False)\n",
    "        syn_subset = synthetic_embeddings[syn_indices]\n",
    "        syn_labels_subset = synthetic_labels[syn_indices]\n",
    "        \n",
    "        for train_idx, val_idx in skf.split(X_train, y_train):\n",
    "            X_cv_train, X_cv_val = X_train[train_idx], X_train[val_idx]\n",
    "            y_cv_train, y_cv_val = y_train[train_idx], y_train[val_idx]\n",
    "            \n",
    "            X_cv_aug = np.concatenate([X_cv_train, syn_subset])\n",
    "            y_cv_aug = np.concatenate([y_cv_train, syn_labels_subset])\n",
    "            \n",
    "            X_cv_aug_scaled = scaler.fit_transform(X_cv_aug)\n",
    "            X_cv_val_scaled = scaler.transform(X_cv_val)\n",
    "            \n",
    "            svm_cv = SVC(C=2.0, kernel='rbf', random_state=42)\n",
    "            svm_cv.fit(X_cv_aug_scaled, y_cv_aug)\n",
    "            cv_scores.append(accuracy_score(y_cv_val, svm_cv.predict(X_cv_val_scaled)))\n",
    "        \n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'f1_score': f1,\n",
    "            'cv_mean': np.mean(cv_scores),\n",
    "            'cv_std': np.std(cv_scores),\n",
    "            'model': svm_aug,\n",
    "            'scaler': scaler\n",
    "        }\n",
    "    \n",
    "    def _generate_test_predictions(self, audio_embedder, text_embedder, fusion_model, \n",
    "                                 trained_svm, scaler, config_name):\n",
    "        \"\"\"Generate test predictions.\"\"\"\n",
    "        print(f\"Generating test predictions for {config_name}...\")\n",
    "        \n",
    "        # Create test embeddings\n",
    "        test_embeddings = []\n",
    "        test_dataset = self.test_loader.load_dataset(align_modalities=True)\n",
    "        \n",
    "        for _, row in tqdm(test_dataset.iterrows(), desc=\"Processing test data\", total=len(test_dataset), leave=False):\n",
    "            try:\n",
    "                subject_chunks = self.test_loader.get_subject_chunks(row['subject_id'])\n",
    "                audio_chunks = subject_chunks['participant_chunks']\n",
    "                text = row['participant_text'] if pd.notna(row.get('participant_text')) else \"\"\n",
    "                \n",
    "                if audio_chunks and len(audio_chunks) > 0:\n",
    "                    audio_emb = audio_embedder.extract_embeddings(audio_chunks).mean(axis=0)\n",
    "                    if text and len(text.strip()) > 0:\n",
    "                        text_emb = text_embedder.extract_embeddings([text])[0]\n",
    "                    else:\n",
    "                        text_emb = np.zeros(768)  # Default if no text\n",
    "                    \n",
    "                    audio_tensor = torch.tensor(audio_emb, dtype=torch.float32).to(self.device)\n",
    "                    text_tensor = torch.tensor(text_emb, dtype=torch.float32).to(self.device)\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        fused_emb = fusion_model(audio_tensor.unsqueeze(0), text_tensor.unsqueeze(0))\n",
    "                    \n",
    "                    test_embeddings.append(fused_emb.squeeze(0).cpu().numpy())\n",
    "                else:\n",
    "                    # Fallback for missing audio\n",
    "                    test_embeddings.append(np.zeros(512))\n",
    "                    \n",
    "            except Exception:\n",
    "                test_embeddings.append(np.zeros(512))\n",
    "        \n",
    "        # Generate predictions\n",
    "        X_test = np.array(test_embeddings)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        test_preds = trained_svm.predict(X_test_scaled)\n",
    "        \n",
    "        # Save in ADReSS format\n",
    "        filename = f'test_results_generation_{config_name}.txt'\n",
    "        with open(filename, 'w') as f:\n",
    "            f.write('ID   ; Prediction\\n')\n",
    "            for i, pred in enumerate(test_preds):\n",
    "                subject_id = f'S{i+160:03d}'\n",
    "                f.write(f'{subject_id} ; {pred}\\n')\n",
    "        \n",
    "        print(f\"Test predictions saved to {filename}\")\n",
    "        \n",
    "        return {\n",
    "            'predictions': test_preds,\n",
    "            'filename': filename\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69687751",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_generation_results(result, synthetic_samples):\n",
    "    \"\"\"Create plots for text-audio generation results.\"\"\"\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    fig.suptitle(f'Text→Audio Generation Results: {result[\"config\"]}', fontsize=16)\n",
    "    \n",
    "    # Performance comparison\n",
    "    metrics = ['baseline_accuracy', 'augmented_accuracy', 'baseline_f1', 'augmented_f1']\n",
    "    values = [result[m] for m in metrics]\n",
    "    colors = ['lightblue', 'darkblue', 'lightgreen', 'darkgreen']\n",
    "    \n",
    "    bars = axes[0, 0].bar(range(len(metrics)), values, color=colors, alpha=0.7)\n",
    "    axes[0, 0].set_title('Performance Comparison')\n",
    "    axes[0, 0].set_xticks(range(len(metrics)))\n",
    "    axes[0, 0].set_xticklabels([m.replace('_', ' ').title() for m in metrics], rotation=45)\n",
    "    axes[0, 0].set_ylabel('Score')\n",
    "    \n",
    "    for i, (bar, value) in enumerate(zip(bars, values)):\n",
    "        axes[0, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                       f'{value:.3f}', ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    # Sample text lengths by severity\n",
    "    text_lengths = {}\n",
    "    severities = ['healthy', 'mild', 'moderate']\n",
    "    \n",
    "    for severity in severities:\n",
    "        lengths = [len(s['text'].split()) for s in synthetic_samples if s['severity'] == severity]\n",
    "        text_lengths[severity] = lengths\n",
    "    \n",
    "    axes[0, 1].boxplot([text_lengths[s] for s in severities], labels=severities)\n",
    "    axes[0, 1].set_title('Generated Text Lengths by Severity')\n",
    "    axes[0, 1].set_ylabel('Words per Sample')\n",
    "    \n",
    "    # MMSE distribution\n",
    "    mmse_healthy = [s['mmse'] for s in synthetic_samples if s['label'] == 0]\n",
    "    mmse_ad = [s['mmse'] for s in synthetic_samples if s['label'] == 1]\n",
    "    \n",
    "    axes[0, 2].hist(mmse_healthy, alpha=0.7, label='Healthy', bins=15, color='green')\n",
    "    axes[0, 2].hist(mmse_ad, alpha=0.7, label='AD', bins=15, color='red')\n",
    "    axes[0, 2].set_title('MMSE Distribution in Generated Samples')\n",
    "    axes[0, 2].set_xlabel('MMSE Score')\n",
    "    axes[0, 2].set_ylabel('Count')\n",
    "    axes[0, 2].legend()\n",
    "    \n",
    "    # Audio duration distribution\n",
    "    audio_durations = [len(s['audio']) / 16000 for s in synthetic_samples]  # Convert to seconds\n",
    "    axes[1, 0].hist(audio_durations, bins=20, alpha=0.7, color='purple')\n",
    "    axes[1, 0].set_title('Generated Audio Durations')\n",
    "    axes[1, 0].set_xlabel('Duration (seconds)')\n",
    "    axes[1, 0].set_ylabel('Count')\n",
    "    \n",
    "    # Quality metrics\n",
    "    quality_keys = ['frechet_distance', 'ks_statistic']\n",
    "    quality_values = [result[k] for k in quality_keys if k in result]\n",
    "    if quality_values:\n",
    "        axes[1, 1].bar(range(len(quality_keys)), quality_values, alpha=0.7, color='orange')\n",
    "        axes[1, 1].set_title('Synthetic Data Quality')\n",
    "        axes[1, 1].set_xticks(range(len(quality_keys)))\n",
    "        axes[1, 1].set_xticklabels([k.replace('_', ' ').title() for k in quality_keys])\n",
    "        axes[1, 1].set_ylabel('Score (lower is better)')\n",
    "    \n",
    "    # Improvement visualization\n",
    "    improvement = result['improvement']\n",
    "    color = 'green' if improvement > 0 else 'red'\n",
    "    axes[1, 2].bar(['Improvement'], [improvement], color=color, alpha=0.7)\n",
    "    axes[1, 2].set_title('Performance Improvement')\n",
    "    axes[1, 2].set_ylabel('Accuracy Change')\n",
    "    axes[1, 2].axhline(y=0, color='black', linestyle='-', alpha=0.5)\n",
    "    axes[1, 2].text(0, improvement + 0.01 if improvement > 0 else improvement - 0.01,\n",
    "                   f'{improvement:+.3f}', ha='center', va='bottom' if improvement > 0 else 'top',\n",
    "                   fontweight='bold', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'generation_results_{result[\"config\"]}.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def display_sample_generations(synthetic_samples, num_samples=3):\n",
    "    \"\"\"Display sample generated text-audio pairs.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SAMPLE GENERATED TEXT-AUDIO PAIRS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for i, sample in enumerate(synthetic_samples[:num_samples]):\n",
    "        print(f\"\\nSample {i+1}:\")\n",
    "        print(f\"Label: {'AD' if sample['label'] == 1 else 'Healthy'}\")\n",
    "        print(f\"MMSE: {sample['mmse']:.2f}\")\n",
    "        print(f\"Severity: {sample['severity']}\")\n",
    "        print(f\"Text: \\\"{sample['text']}\\\"\")\n",
    "        print(f\"Audio duration: {len(sample['audio'])/16000:.2f} seconds\")\n",
    "        print(f\"Text length: {len(sample['text'].split())} words\")\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5638a26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution for text→audio generation pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Set seeds\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"TEXT→AUDIO GENERATION PIPELINE FOR AD DETECTION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Load datasets\n",
    "    print(\"Loading ADReSS datasets...\")\n",
    "    \n",
    "    # Paths (adjust for your environment)\n",
    "    train_path = \"ADReSS-IS2020-train/ADReSS-IS2020-data/train\"  # Local\n",
    "    test_path = \"ADReSS-IS2020-test/ADReSS-IS2020-data/test\"\n",
    "    # train_path = \"/kaggle/input/adress/adress/train\"  # Kaggle\n",
    "    # test_path = \"/kaggle/input/adress/adress/test\"\n",
    "    \n",
    "    train_config = ADReSSConfig(train_path=train_path)\n",
    "    test_config = ADReSSConfig(train_path=test_path)\n",
    "    \n",
    "    train_loader = ADReSSDataLoader(train_path, train_config)\n",
    "    test_loader = ADReSSDataLoader(test_path, test_config)\n",
    "    \n",
    "    # Initialize pipeline\n",
    "    pipeline = ExperimentPipeline(train_loader, test_loader, device)\n",
    "    \n",
    "    # Single configuration: ClinicalBERT + Whisper + Cross Attention\n",
    "    config = {'audio_model': 'whisper', 'text_model': 'clinicalbert', 'fusion_type': 'cross_attention'}\n",
    "    \n",
    "    print(\"Running ClinicalBERT + Whisper + Cross Attention generation experiment...\")\n",
    "    print(f\"Configuration: {config}\")\n",
    "    \n",
    "    try:\n",
    "        # Run experiment\n",
    "        result = pipeline.run_generation_experiment(\n",
    "            audio_model=config['audio_model'],\n",
    "            text_model=config['text_model'], \n",
    "            fusion_type=config['fusion_type'],\n",
    "            samples_per_class=100  # Generate more samples for single config\n",
    "        )\n",
    "        \n",
    "        # Generate sample data for visualization\n",
    "        generation_config = GenerationConfig(num_samples_per_condition=10)\n",
    "        data_generator = AlignedDataGenerator(generation_config, device)\n",
    "        sample_synthetic = data_generator.generate_balanced_dataset(10)\n",
    "        \n",
    "        # Store results\n",
    "        results = [result]\n",
    "        all_synthetic_samples = {result['config']: sample_synthetic}\n",
    "        \n",
    "        # Create plots\n",
    "        plot_generation_results(result, sample_synthetic)\n",
    "        \n",
    "        # Display sample generations\n",
    "        display_sample_generations(sample_synthetic)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in experiment: {e}\")\n",
    "        results = []\n",
    "    \n",
    "    print(\"\\\\n\" + \"=\"*60)\n",
    "    print(\"EXPERIMENT SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if results:\n",
    "        # Single result\n",
    "        result = results[0]\n",
    "        \n",
    "        # Save result\n",
    "        results_df = pd.DataFrame([result])\n",
    "        results_df.to_csv('text_audio_generation_results.csv', index=False)\n",
    "        \n",
    "        print(f\"\\nExperiment completed successfully!\")\n",
    "        print(\"Results saved to text_audio_generation_results.csv\")\n",
    "        \n",
    "        # Display summary\n",
    "        print(\"\\nRESULT SUMMARY:\")\n",
    "        print(\"-\" * 40)\n",
    "        print(f\"Configuration: {result['config']}\")\n",
    "        print(f\"  Baseline: {result['baseline_accuracy']:.3f}\")\n",
    "        print(f\"  Augmented: {result['augmented_accuracy']:.3f}\")\n",
    "        print(f\"  Improvement: {result['improvement']:+.3f}\")\n",
    "        print(f\"  Synthetic samples: {result['num_synthetic_samples']}\")\n",
    "        print(f\"  Fréchet Distance: {result['frechet_distance']:.3f}\")\n",
    "        \n",
    "        # Create simple summary plot\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        metrics = ['Baseline', 'Augmented']\n",
    "        values = [result['baseline_accuracy'], result['augmented_accuracy']]\n",
    "        colors = ['lightblue', 'darkblue']\n",
    "        bars = plt.bar(metrics, values, color=colors, alpha=0.7)\n",
    "        plt.title('Performance Comparison')\n",
    "        plt.ylabel('Accuracy')\n",
    "        for bar, value in zip(bars, values):\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                    f'{value:.3f}', ha='center', va='bottom', fontsize=12)\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        improvement = result['improvement']\n",
    "        color = 'green' if improvement > 0 else 'red'\n",
    "        plt.bar(['Improvement'], [improvement], color=color, alpha=0.7)\n",
    "        plt.title('Performance Improvement')\n",
    "        plt.ylabel('Accuracy Change')\n",
    "        plt.axhline(y=0, color='black', linestyle='-', alpha=0.5)\n",
    "        plt.text(0, improvement + 0.01 if improvement > 0 else improvement - 0.01,\n",
    "                f'{improvement:+.3f}', ha='center', va='bottom' if improvement > 0 else 'top',\n",
    "                fontweight='bold', fontsize=12)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('whisper_clinicalbert_generation_results.png', dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"\\nGenerated plots and CSV file contain detailed results.\")\n",
    "        \n",
    "    else:\n",
    "        print(\"Experiment failed.\")\n",
    "        print(\"Check error messages above for debugging.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
